# Introduction to Machine Learning
Machine learning is the science of training computers to perform tasks on their own.

One way to classify machine learning:
- **supervised**
- **semi-supervised**
- **unsupervised** and,
- **reinforcement learning**.

In supervised there's classification and regression. The data used here is usually labelled.
**Classification**: the model groups new features based on what it finds common in the given features which it was trained on.
**Regression**: the model will output a numeric value based on the given features and labels it is fed during training.
Other supervised learning algorithms that I will be implementing include:
 - K Nearest Neighbors
 - Logistic regression
 - Linear regression
 - Support vector machines
 - Decision Trees and Random Forests
 - Neural networks

In Unsupervised learning, the data has no pre-defined labels and therefore these algorithms are used to uncover the patterns that would not be easily detected by human beings.
**Clustering** is one of the algorithms in this type of learning. The algorithm is fed data that has no labels and it uncovers the patterns that connects the different data and clusters the data hence the name. 
Other unsupervised algorithms that I will be implementing include:
 - Clustering: 
    * K Means
    * DBSCAN
    * HCA
 - Anomaly detection and novelty detection:
    * One-class SVM
    * Isolation Forest
 - Visualization and dimensionality reduction:
    * Principal Component Analysis (PCA)
    * Kernel PCA
    * Locally Linear Embedding (LLE)
    * t-Distributed Stochastic Neighbor Embedding(t-SNE)
 - Association rule learning
    * Apriori
    * Eclat

Reinforcement learning is whereby the agent learns with minimal human intervention. This means that the agent observes its environment, performs a certain action based on what it  has perceived on the environment and is either rewarded or penalized based on whether its output is correct or not. 

Another way to classify machine learning:
 - **Online Learning**
 - **Batch Learning**

Batch learning: The model will be trained using all the available data. This is time-consuming and therefore it is done offline. The model is then deployed and is run the way it has already been trained without it learning anything anymore.

Online learning: The model learns incrementally in "mini-batches" and it learns as soon as the data is made available. This type of learning is great for systems that need to adapt to changes rapidly. Since the data does not need to be stored in this type of learning, it is a good option when there is a limit in resources.
In online learning, a parameter known as **learning rate** is defined as how fast or slow the system is learning. If this parameter is too high, the system tends to forget faster and if it is too slow, then the system will take a very long time to actually learn, making it less sensitive to new data and it willtake a long time to be able to make accurate predictions.

The last way to categorize machine learning learning: 
 - Model-Based learning
 - Instance-based learning.

 **Instance-Based**: The algorithm learns the material by heart and then forms similarity measure and then uses this to map where new data would be categorised as.

**Model-Based**: Here, we build a model and it foems the basis for predictions. So you:
    1. You study the data
    2. You select the model
    3. You train the model using the data
    4. You then apply the model to make predictions based on what it has learnt. This is called *inferencing*

## Main Challenges of Machine Learning

1. Insuffient Quantity of Data
2. Nonrepresentative Training data 
- data that does not contain the groups for whom you want to make predictions for.
- **Sampling noise**: this is when the training data is so small, it introduces unwanted variations and inaccuracies.
- **Sampling Bias**: Is an error whereby the training data consistently favours a particular group or a certain outcome.
3. Poor Data Quality
- Data full of noise, errors and outliers makes it hard for the systems to predict accurate outcomes.
4. Irrelevant Features
- **Feature engineering**, the process of ensuring relevant(good set of) features are selected for systems to learn with involves:
    - **Feature Selection**: Selecting the appropriate features to train the system on
    - **Feature extraction**: extracting other features from already existing features.
    - Creating new features from existing features.
5. Overfitting
- This is when the system fits the outliers do perfectly as though they were true patterns when in reality they have minimal relation to the other data.
- This leads to poor generalisation as it performs so well on training data but does not do well with new, unseen data.
6. Underfitting the Data
- Happens when the model is too simple to capture the underlying structure of the data. This often leads to errors in both the training data and the validation data.

## Testing and Validation
- This is done to check how well a model generalises given new cases(data).
- You can split the data into **training data** and **testing data**.
- The error from the newly generalised cases is *generalisation error*. If it is high, then your model is overfitting the data.

### Hyperparameter Tuning and Model Selection
- A *hyperparameter* is an external setting or a constant that is set before training begins and is used to control the learning process. *Hyperparameter Tuning* is the iterative process of selecting pre-controlled parameters that will control the learning process.
- *Holdout validation*: holding a set of data(*validation set* or *development set*(*dev-set*)) to use in order to help with selecting the correct model.

### Data Mismatch
- You might have a lot of data but it is not the right data to use for a specific task.



