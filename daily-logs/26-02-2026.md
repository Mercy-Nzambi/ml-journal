# MNIST-like Dataset
- This is a dataset found in the scikit library called datasets, on digits similar to the MNIST one but highly compressed and low resolution, making it easy for beginners to use.
- The following are the steps I have taken for the MNISTlike project.

### 1. Import libraries
    - I used scikit-learn and matplotlib.
    - For scikit-learn or sklearn, I imported specific functions to use.
### 2. Load dataset
    - Using the datasets library in sklearn, I used the digits dataset
### 3. Display the shape
    - This is getting to see the features and labels present in our dataset.
    - There were (1797, 64) features and (1797, ) labels
### 4. Create, train and predict
    - Created the model using the MLPClassifier. Why? Because it can model complex relationships in binary labels like what I am working on.
    - Split the data into testing and training data and train the model using the training subset.
    - Predict the model's output using the features in the testing subset(X_test)
### 5. Evaluate the prediction
    - Check the accuracy score of the model(divide the predicted labels and the actual labels) to determine the efficiency of the model.
### 6. Visualization
    - How do we determine what visual to use, by determining:
        - The purpose of the visual
        - The type of data you are working on
    - The purpose of my visual was to see what number is represented by the pixels in the image and therefore detrmine whether the prediction done by the model is accurate.
    - Before displaying the images, reshapinng the data helps to convert it from the flat 1D vector format the model understands/uses; the shape(64,) to a 2D format that is human readable/understandable reshape(8, 8).

## Insights
- Increasing the number of hidden layers, increases the accuracy of the model up to a certain point where increasing it any further leads to slow performance.
- The MLPClassifier used ignores spatial relationships but works well with a smaller number of pixels because it is able to learn the patterns in the 64D space. This model would struggle with a 784D space because **an increase in the number of features** leads to **an increase in the number of parameters** and means:
    - *more weights*,
    - *more complexity* and 
    - *a higher chance of overfitting*.